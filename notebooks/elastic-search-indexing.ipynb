{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5582083-d519-4b96-9adf-fb05ce1dd62d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized environment variables listed in: /mnt/workspace/__ing/llming/DTC/audio_podcast_qa_assistant/.env\n",
      "Initialized environment variables listed in: /mnt/workspace/__ing/llming/DTC/audio_podcast_qa_assistant/.env\n",
      "Connected to Elasticsearch\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "## replace with root project dir\n",
    "PROJECT_DIR = \"/mnt/workspace/__ing/llming/DTC/audio_podcast_qa_assistant\"\n",
    "sys.path.append(PROJECT_DIR)\n",
    "\n",
    "from utils.utils import (load_json_document,\n",
    "                         initialize_env_variables)\n",
    "\n",
    "from utils.elasticsearch import (\n",
    "    create_elasticsearch_client,\n",
    "    create_elasticsearch_index,\n",
    "    search_elasticsearch_indecis,\n",
    "    load_index_settings,\n",
    "    remove_elasticsearch_index,\n",
    "    index_document,\n",
    "    get_index_mapping,\n",
    "    get_indexed_documents_count,\n",
    ")\n",
    "\n",
    "from utils.ollama import (get_embedding,\n",
    "                          embed_document, create_ollama_client)\n",
    "from utils.multithread import map_progress\n",
    "\n",
    "initialize_env_variables()\n",
    "\n",
    "from utils.query import (elastic_search_text, llm, elastic_search_knn,\n",
    "                         build_context, build_prompt,\n",
    "                         ES_CLIENT, OLLAMA_CLIENT, OPENAI_CLIENT)\n",
    "from utils.ollama import get_embedding\n",
    "from utils.utils import flatten_list_of_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c62ced-1e67-4f11-9bbb-cf76ef4618ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d17aadf7-421f-4b7e-b1e9-19afc612e6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lex-fridman-podcast']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Client creation\n",
    "es_host = os.getenv('ELASTIC_SETUP_HOST')\n",
    "es_port = os.getenv('ELASTIC_PORT')\n",
    "\n",
    "index_name = \"lex-fridman-podcast\"\n",
    "index_settings_path=f\"{PROJECT_DIR}/config/elasticsearch/index_settings.json\"\n",
    "index_settings = load_index_settings(index_settings_path)\n",
    "\n",
    "es_client = create_elasticsearch_client(es_host, es_port)\n",
    "search_elasticsearch_indecis(es_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e204744-4f70-4c00-afda-6032b82befbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found an existing index with name lex-fridman-podcast, nothing to do.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lex-fridman-podcast']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_elasticsearch_index(es_client, index_name, index_settings)\n",
    "search_elasticsearch_indecis(es_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "595f3ae8-62e8-47ca-a51e-51242c7881a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.path.join(PROJECT_DIR, \"data/generated_documents/documents.json\")\n",
    "documents = load_json_document(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1df702ee-a72b-4eb9-97a5-d6c25108f6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ollama_host = os.getenv('OLLAMA_SETUP_HOST')\n",
    "ollama_port = os.getenv('OLLAMA_PORT')\n",
    "\n",
    "ollama_client = create_ollama_client(ollama_host, ollama_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cd5ba5e-e536-4ef6-8cd3-0178a937cb94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1473d5c0268b4f2a9a3413f2923aae36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## indexing\n",
    "embed_model_name = os.getenv('EMBED_MODEL')\n",
    "\n",
    "vectorized_documents = map_progress(\n",
    "    f=lambda document: embed_document(\n",
    "        ollama_client, document, embed_model_name),\n",
    "    seq=documents,\n",
    "    max_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f93985c-cc50-4eba-9637-7a18cdd18c78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_to_pickle(obj, pickle_file_path):\n",
    "    \"\"\"\n",
    "    Saves a Python object to a file using pickle.\n",
    "    \n",
    "    :param obj: The Python object to be pickled.\n",
    "    :param pickle_file_path: Path where the pickled object will be saved.\n",
    "    \"\"\"\n",
    "    with open(pickle_file_path, 'wb') as pickle_file:\n",
    "        pickle.dump(obj, pickle_file)\n",
    "        \n",
    "pickle_file_path = os.path.join(\n",
    "    PROJECT_DIR, \"data/generated_document_embeddings/embeddings.pkl\")\n",
    "\n",
    "\n",
    "save_to_pickle(vectorized_documents, pickle_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81148080-772e-4c62-a76d-6e94e3915db8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = map_progress(\n",
    "    f=lambda vectorized_document: index_document(\n",
    "        es_client, index_name, vectorized_document),\n",
    "    seq=vectorized_documents,\n",
    "    max_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe674fe8-e11e-4eb7-9633-578356b2d9c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'count': 22232, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_indexed_documents_count(es_client, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5027c1f5-2be8-42b7-911f-bca2ddd0b688",
   "metadata": {},
   "source": [
    "Indexed all documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8028fbe-f083-4d69-999a-73b67a1badc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e049a5b8-257a-4c96-856a-03f14c614d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titles = list(set([document['title'] for document in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02226f0a-546f-4443-8231-8d56183fa11f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: What is the leadership of the Paris Commune going to do? And why? And in what order? In other words, governing, organizing a society. But since it only lasted a few weeks, the French army regrouped, and under the leadership of people who were very opposed to Marx, they marched back into Paris, took over, killed a large number of the communards, as they were called, and deported them to islands in the Pacific that were part of the French Empire at the time. The really big change happens in Russia in 1917. Now you have a group of Marxists, Lenin, Trotsky, all the rest, who are in this bizarre position to seize a moment. Once again, a war, like in France, disorganizes the government, throws the government into a very bad reputation, because it is the government that loses World War I, has to withdraw, as you know, Brest-Litovsk and all of that, and the government collapses, and the army revolts. And in that situation, a very small political party, Russian social democratic workers party, splits under the pressures of all of this into the Bolshevik and Menshevik divisions. Lenin, Trotsky, and the others are in the Bolshevik division. And to make a long story short, he's in exile. Lenin's position makes him, gets him deported, because he says Russian workers should not be killing German workers. I mean, this is a war of capitalists who are dividing the world up into colonies, and Russian working people should not kill and should not die for such a thing. As you can expect, they arrest him and they throw him out. Interestingly, in the United States, the comparable leader at that time of the Socialist Party here, as you know, there was no Communist Party at this point, that comes later, the head of the Socialist Party, a very important American figure named Eugene Victor Debs, makes exactly the same argument to the Americans should not fight in the war. He's in the, he has nothing to do with Lenin, I don't even know if they knew of each other, but he does it on his own.\n",
      "Title: Richard Wolff: Marxism and Communism | Lex Fridman Podcast #295\n",
      "Chunk ID: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the query\n",
    "title_query = \"communism \"\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"title\": {\n",
    "                            \"query\": title_query,\n",
    "                            \"fuzziness\": \"AUTO\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"_source\": [\"text\", \"title\", \"chunk_id\"]\n",
    "}\n",
    "\n",
    "# Perform the search\n",
    "response = es_client.search(index=index_name, body=query)\n",
    "\n",
    "# Extract and print the results\n",
    "for hit in response['hits']['hits']:\n",
    "    text = hit['_source'].get('text', 'No text field found')\n",
    "    title = hit['_source'].get('title', 'No title field found')\n",
    "    chunk_id = hit['_source'].get('chunk_id', 'No chunk_id field found')\n",
    "    print(f\"Text: {text}\\nTitle: {title}\\nChunk ID: {chunk_id}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dafd5dd-fab1-4bb4-ab3b-3ed19f5c5954",
   "metadata": {},
   "source": [
    "# Testing RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368518fb-d1b5-4d7f-a14e-8d7b0730a7ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9482f491-0289-4d18-a2e9-3d9204809feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('No, Jed Buchwald does not believe that science progresses via paradigm shifts and revolutions as philosopher Thomas Kuhn said. He thinks that while paradigm shifts exist, the changes happen more complexly and not as neatly in reaction to experimental observations. He also believes that there is a mix of individual lone geniuses and messy collaboration of competing and cooperating humans in the progression of science.',\n",
       " {'prompt_tokens': 2638, 'completion_tokens': 75, 'total_tokens': 2713},\n",
       " 2.497809648513794)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"What're the pros and cons of communism vs capitalism?\"\n",
    "query = \"According to Jed Buchwald, Does science progress via paradigm shifts and revolutions as philosopher Thomas Kuhn said, or does it progress gradually?\"\n",
    "title_query = \"communism and capitalism\"\n",
    "search_results = elastic_search_text(query, title_query)\n",
    "context = build_context(search_results)\n",
    "document_dict = {\"question\": query, \"context\": context}\n",
    "prompt = build_prompt(**document_dict)\n",
    "llm(prompt, model_choice=\"openai/gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75137d-a7d3-4e37-9736-028879063746",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ac2cc9-9b4c-40a0-8f03-4ae8b3254e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"No, Jed Buchwald does not completely agree with Thomas Kuhn's view on paradigm shifts and revolutions in science. He believes that while paradigm shifts do exist, they may not be as powerful or neatly defined as Kuhn proposed. Buchwald suggests that changes in science are more complex and are influenced by a combination of individual geniuses and collaborative efforts.\",\n",
       " {'prompt_tokens': 2445, 'completion_tokens': 71, 'total_tokens': 2516},\n",
       " 1.953244686126709)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"What're the pros and cons of communism vs capitalism?\"\n",
    "query = \"According to Jed Buchwald, Does science progress via paradigm shifts and revolutions as philosopher Thomas Kuhn said, or does it progress gradually?\"\n",
    "title_query = \"communism and capitalism\"\n",
    "query_embedding = get_embedding(OLLAMA_CLIENT, query)\n",
    "search_results = elastic_search_knn(\n",
    "    query_embedding, title_query\n",
    ")\n",
    "context = build_context(search_results)\n",
    "document_dict = {\"question\": query, \"context\": context}\n",
    "prompt = build_prompt(**document_dict)\n",
    "llm(prompt, model_choice=\"openai/gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420fcc9e-718f-4a92-b345-df88a3c7cac2",
   "metadata": {},
   "source": [
    "# DUMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34822425-aef7-4b23-a2d2-e51d77fe3d46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76053f1f-88f7-4229-82cc-06f1655740f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'chunk_id', 'channel', 'channel_id', 'title', 'categories', 'tags', 'text', 'text_vector']\n"
     ]
    }
   ],
   "source": [
    "index_settings_path = f\"{PROJECT_DIR}/config/elasticsearch/index_settings.json\"\n",
    "index_settings = load_index_settings(index_settings_path)\n",
    "\n",
    "print(list(index_settings[\"mappings\"][\"properties\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4404af-e4c7-402b-909c-31debbefda87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = os.path.join(PROJECT_DIR, \"data/generated_document_embeddings/embeddings.pkl\")\n",
    "with open(path, 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b502741-0da2-4962-9b01-3fb34e931c44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index_settings.json']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "dir_path = \"/mnt/workspace/__ing/llming/DTC/audio_podcast_qa_assistant/config/elasticsearch\"\n",
    "\n",
    "def get_json_files_in_dir(dir_path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    json_files = glob.glob(\n",
    "        os.path.join(dir_path, '*.json')\n",
    "    )\n",
    "\n",
    "    return [json_file.split('/')[-1] for json_file in json_files]\n",
    "\n",
    "get_json_files_in_dir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d811989-8ab2-4359-b607-19dbf83dae7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.utils import save_json_file, read_json_file, get_json_files_in_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59cd1ea-760e-46fd-97b1-025d9d3c5cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcripts_cache_dir = os.path.join(\n",
    "    PROJECT_DIR,\n",
    "    \"data/generated_transcriptions\"\n",
    ")\n",
    "\n",
    "def load_cached_episodes(transcripts_cache_dir):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for path in get_json_files_in_dir(transcripts_cache_dir, return_full_path=True):   \n",
    "        dataset.append(read_json_file(path))\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828791ab-350c-4be5-9f1a-4eaa07b056dd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'nhGwJLXzHs8',\n",
       "  'channel': 'Lex Fridman',\n",
       "  'channel_id': 'UCSHZKyawb77ixDdsGog4iWA',\n",
       "  'title': 'Brian Keating: Cosmology, Astrophysics, Aliens & Losing the Nobel Prize | Lex Fridman Podcast #257',\n",
       "  'categories': ['Science & Technology'],\n",
       "  'tags': ['agi',\n",
       "   'ai',\n",
       "   'ai podcast',\n",
       "   'artificial intelligence',\n",
       "   'artificial intelligence podcast',\n",
       "   'astrophysics',\n",
       "   'bicep',\n",
       "   'big bang',\n",
       "   'brian keating',\n",
       "   'cosmology',\n",
       "   'lex ai',\n",
       "   'lex fridman',\n",
       "   'lex jre',\n",
       "   'lex mit',\n",
       "   'lex podcast',\n",
       "   'mit ai',\n",
       "   'nobel prize',\n",
       "   'physics',\n",
       "   'science',\n",
       "   'space'],\n",
       "  'text': \"Not like in the high school, like hypothesis, thesis, but just like, wow, how did I feel? Better yet, astronomy is a visual science. Sketch what you see. The Lagoon Nebula, the Pleiades Seven Sisters. You can see them anywhere on earth. And when you do that, again, you're connecting two different hemispheres of your brain, as I understand it, and you're connecting them through your fingertips. You literally have the knowledge in your fingertips, in your connection between what you see, what you observe, and what you write down. Then you do research, right? The goal of science is not to just replicate what other people did, is do something new. And that's what we call it research, and not just like studying, you know, Wikipedia. And in so doing, you start to train a kid at age 12 or 13 for 50 bucks. It's unbelievable. And now we can do even better, because you got to share it on Instagram or whatever, and you can, by doing so, have an entree into the world of what does it really mean to be a scientist, and do so viscerally. You know, I often say, I was taught this by my English teacher, Mrs. Tompkins, in ninth grade, that the word educate, it doesn't mean to pour into. Let me pour in some facts, intellects, and you know, it's not like machine learning, you're just showing like billions of cats, or you know, you're not like forcing it in, you're bringing it out. It means to pour out of, in Latin, educare. And what more could a teacher want than to have something, the kid is just like gushing. No, you're not gonna see like- To inspire the kid. Yes. Inspire. Shout out to Mrs. Tompkins. Mrs. Tompkins, she's watching, yeah. She's a big fan. Me, she doesn't care for, but you. Yeah, excellent. We take those we love for granted. This is in Manhattan. This is in Westchester County, New York. Okay, got it. So, okay, so, but then that's where the dream is born. Yeah. But then there is the pragmatic journey of a scientist.\",\n",
       "  'chunk_id': 21},\n",
       " {'id': 'TRdL6ZzWBS0',\n",
       "  'channel': 'Lex Fridman',\n",
       "  'channel_id': 'UCSHZKyawb77ixDdsGog4iWA',\n",
       "  'title': 'Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214',\n",
       "  'categories': ['Science & Technology'],\n",
       "  'tags': ['agi',\n",
       "   'ai',\n",
       "   'ai podcast',\n",
       "   'artificial intelligence',\n",
       "   'artificial intelligence podcast',\n",
       "   'caltech',\n",
       "   'classic mechanics',\n",
       "   'einstein',\n",
       "   'feynman',\n",
       "   'general relativity',\n",
       "   'jed buchwald',\n",
       "   'lex ai',\n",
       "   'lex fridman',\n",
       "   'lex jre',\n",
       "   'lex mit',\n",
       "   'lex physics',\n",
       "   'lex podcast',\n",
       "   'mit ai',\n",
       "   'physics',\n",
       "   'quantum mechanics',\n",
       "   'thomas kuhn'],\n",
       "  'text': \"Does science progress via paradigm shifts and revolutions as philosopher Thomas Kuhn said, or does it progress gradually? What do you think? Well, I got into this field because I was Tom Kuhn's research assistant 50 years ago, 52 years ago. He pulled me into it out of physics instead. So I know his work pretty well. And in the years when I was at MIT running an institute, he was then in the philosophy department, used to come over all the time to the talks we held and so on. So what would I say about that? He, of course, developed his ideas a lot over the years. The thing that he's famous for, the structure of scientific revolutions came out in 62. And as you just said, it offered an outline for what he called a paradigmatic structure, namely the notion that you have to look at what scientists do as forming a community of investigators. And that they're trying to solve various puzzles, as he would put it, that crop up, figuring out how this works, how that works and so on. And of course, they don't do it out of the blue. They do it within a certain framework. The framework can be pretty vague. He called it a paradigm. And his notion was that eventually they run into troubles or what he called anomalies. That kind of cracks things. Somebody new comes along with a different way of doing it, etc. Do I think things work that way? No, not really. Tom and I used to have lengthy discussions about that over the years. I do think there is a common structure that formulates both theoretical and experimental practices. And historians nowadays of science like to refer to scientific work as what scientists practice. It's almost craftsman-like. They can usually adapt in various ways. And I can give you all kinds of examples of that. I once wrote a book on the origins of wave theory of light. And that is one of the paradigmatic examples that Tom used. Only it didn't work that way exactly.\",\n",
       "  'chunk_id': 0}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faa50bc5-9f7f-414f-b3f6-18f33d312f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method build_from_flow in module prefect.deployments.deployments:\n",
      "\n",
      "build_from_flow(flow: prefect.flows.Flow, name: str, output: str = None, skip_upload: bool = False, ignore_file: str = '.prefectignore', apply: bool = False, load_existing: bool = True, schedules: Optional[Sequence[Union[prefect.client.schemas.objects.MinimalDeploymentSchedule, dict, prefect.client.schemas.schedules.IntervalSchedule, prefect.client.schemas.schedules.CronSchedule, prefect.client.schemas.schedules.RRuleSchedule, prefect.client.schemas.schedules.NoSchedule]]] = None, **kwargs) -> 'Deployment' method of pydantic.v1.main.ModelMetaclass instance\n",
      "    Configure a deployment for a given flow.\n",
      "    \n",
      "    Args:\n",
      "        flow: A flow function to deploy\n",
      "        name: A name for the deployment\n",
      "        output (optional): if provided, the full deployment specification will be\n",
      "            written as a YAML file in the location specified by `output`\n",
      "        skip_upload: if True, deployment files are not automatically uploaded to\n",
      "            remote storage\n",
      "        ignore_file: an optional path to a `.prefectignore` file that specifies\n",
      "            filename patterns to ignore when uploading to remote storage; if not\n",
      "            provided, looks for `.prefectignore` in the current working directory\n",
      "        apply: if True, the deployment is automatically registered with the API\n",
      "        load_existing: if True, load any settings that may already be configured for\n",
      "            the named deployment server-side (e.g., schedules, default parameter\n",
      "            values, etc.)\n",
      "        schedules: An optional list of schedules. Each item in the list can be:\n",
      "              - An instance of `MinimalDeploymentSchedule`.\n",
      "              - A dictionary with a `schedule` key, and optionally, an\n",
      "                `active` key. The `schedule` key should correspond to a\n",
      "                schedule type, and `active` is a boolean indicating whether\n",
      "                the schedule is active or not.\n",
      "              - An instance of one of the predefined schedule types:\n",
      "                `IntervalSchedule`, `CronSchedule`, or `RRuleSchedule`.\n",
      "        **kwargs: other keyword arguments to pass to the constructor for the\n",
      "            `Deployment` class\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prefect.deployments import Deployment\n",
    "\n",
    "help(Deployment.build_from_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c54a924c-9bb5-440a-9c27-c3880aa19acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool([1] or False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8fecb4b-5684-4ee7-8d15-451b12e57ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22232"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_indexed_documents_count(es_client,index_name)['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25477350-f873-43d4-81f8-64878d03ab36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ids_response = es_client.search(index=index_name, body={\n",
    "    \"query\": {\n",
    "        \"term\": {\n",
    "            \"id\": \"L_Guz73e6fw\"\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "# Extract document IDs from the search results\n",
    "_ids = [hit[\"_id\"] for hit in _ids_response[\"hits\"][\"hits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3834e3c-bf2c-4bbc-9a3f-53906147ad5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GxsmW5EB5QWlJbpY5YDk']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c846f9bb-e068-4da1-99a5-5200e81f2fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = es_client.delete(index=index_name, id=\"GxsmW5EB5QWlJbpY5YDk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc2b2edd-f661-4ffc-8124-977c7d12d501",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_index': 'lex-fridman-podcast', '_id': 'FhvxWpEB5QWlJbpYdYB3', '_version': 2, 'result': 'deleted', '_shards': {'total': 1, 'successful': 1, 'failed': 0}, '_seq_no': 22240, '_primary_term': 16})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f186b1-d93a-45be-ada5-5f05e592c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d8d36-6a4d-4829-802a-96fc748a67e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 (dtc-llm-env)",
   "language": "python",
   "name": "dtc-llm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
