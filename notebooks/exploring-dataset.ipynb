{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "796ac5aa-fd59-4c94-a1a1-1bdca4a97fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized environment variables listed in: /mnt/workspace/__ing/llming/DTC/audio_podcast_qa_assistant/.env\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "from scipy.io.wavfile import write\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## replace with root project dir\n",
    "PROJECT_DIR = \"/mnt/workspace/__ing/llming/DTC/audio_podcast_qa_assistant\"\n",
    "sys.path.append(PROJECT_DIR)\n",
    "\n",
    "from utils.utils import (\n",
    "    initialize_env_variables,\n",
    ")\n",
    "\n",
    "## HF_HOME\n",
    "cache_dir = os.path.join(PROJECT_DIR, \"hf_cache\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "initialize_env_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130b8093-3f88-4c9b-bd74-4ec244d70ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/mohammed/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(os.getenv(\"HF_READING_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac896ae-cc1e-43f4-815e-d51d2dc5b639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f131356c24454481bb0ea87e3009b49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0bf21f3e3846dca77737fa14b9e083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the full lex fridman podcast dataset\n",
    "full_dataset = load_dataset(\n",
    "    'Whispering-GPT/lex-fridman-podcast-transcript-audio', \n",
    "    cache_dir=cache_dir,\n",
    "    ignore_verifications=True\n",
    ")['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a183a-4205-4e26-8bdc-0a26fb43558a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ad332f-dd56-4c96-b4f7-2433157fb85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", cache_dir=cache_dir)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\", cache_dir=cache_dir)\n",
    "model.config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5e3f0e-a87c-4d5f-adc9-c270cc39742e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## sample minutes of audio\n",
    "def sample_audio(\n",
    "    audio_dict, start_from=None, minutes=None\n",
    "):\n",
    "    audio = audio_dict['array']\n",
    "    sampling_rate = audio_dict['sampling_rate']\n",
    "    \n",
    "    start_from = int(start_from*60*sampling_rate) if start_from else 0\n",
    "    up_to = start_from + int(minutes*60*sampling_rate) if minutes else None\n",
    "        \n",
    "    return audio[start_from:up_to]\n",
    "\n",
    "## Change sampling_rate\n",
    "def update_sampling_rate(audio_data, original_rate, target_rate):\n",
    "    num_samples = round(len(audio_data) * float(target_rate) / original_rate)\n",
    "    resampled_audio = resample(audio_data, num_samples)\n",
    "    return resampled_audio\n",
    "\n",
    "## Transcripe audio\n",
    "def transcripe_audio(\n",
    "    audio,\n",
    "    processor,\n",
    "    model,\n",
    "    sampling_rate=16_000,\n",
    "    skip_special_tokens=True,\n",
    "):\n",
    "    input_features = processor(\n",
    "        audio, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "\n",
    "    predicted_ids = model.generate(input_features)\n",
    "\n",
    "    return processor.batch_decode(\n",
    "        predicted_ids, skip_special_tokens=skip_special_tokens\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_transcripts(transcripts):\n",
    "    \"\"\"\n",
    "    Merges a list of transcripts into a coherent and grammatically correct speech.\n",
    "\n",
    "    Args:\n",
    "        transcripts (list of str): List of transcribed text chunks.\n",
    "\n",
    "    Returns:\n",
    "        str: Merged and coherent speech.\n",
    "    \"\"\"\n",
    "    merged_transcript = \"\"\n",
    "    \n",
    "    for i, transcript in enumerate(transcripts):\n",
    "        # Strip leading/trailing whitespace from the transcript\n",
    "        transcript = transcript.strip()\n",
    "        \n",
    "        # If it's not the first chunk, try to merge smoothly with the previous one\n",
    "        if i > 0:\n",
    "            # Handle cases where the previous chunk ends in an incomplete sentence\n",
    "            if re.match(r'^\\w', transcript):\n",
    "                # Append a space if the transcript starts with a word character (alphanumeric)\n",
    "                merged_transcript += \" \" + transcript\n",
    "            else:\n",
    "                # Directly append if the transcript starts with punctuation\n",
    "                merged_transcript += transcript\n",
    "        else:\n",
    "            merged_transcript += transcript\n",
    "    \n",
    "    # Final pass to fix any spacing issues\n",
    "    merged_transcript = re.sub(r'\\s+', ' ', merged_transcript).strip()\n",
    "    \n",
    "    return merged_transcript\n",
    "\n",
    "\n",
    "## Transcripe full episode\n",
    "def transcripe_episode(\n",
    "    episode,\n",
    "    processor,\n",
    "    model,\n",
    "    skip_special_tokens=True,\n",
    "    **sampling_kwargs,\n",
    "):\n",
    "    minutes=sampling_kwargs.get('minutes', 2)\n",
    "    target_sampling_rate=sampling_kwargs.get('target_sampling_rate', 16_000)\n",
    "    \n",
    "    transcripts_list = []\n",
    "    episode_length = len(episode['array'])\n",
    "    orig_sampling_rate = episode['sampling_rate']\n",
    "    \n",
    "    \n",
    "    # Calculate the total duration of the episode in seconds\n",
    "    episode_duration_seconds = episode_length / orig_sampling_rate\n",
    "    \n",
    "    # Calculate the list of start_from values in minutes (could be float)\n",
    "    start_froms = [i * minutes for i in range(int(episode_duration_seconds // (minutes * 60)) + 1)]\n",
    "    \n",
    "    for start_from in tqdm(start_froms):\n",
    "        audio = sample_audio(episode, start_from=start_from ,minutes=minutes)      \n",
    "        audio = update_sampling_rate(audio, orig_sampling_rate, target_sampling_rate)\n",
    "        transcripts_list += transcripe_audio(audio, processor, model, target_sampling_rate)\n",
    "    \n",
    "    return merge_transcripts(transcripts_list)\n",
    "\n",
    "\n",
    "def get_resuming_index(data_dir):\n",
    "    filenames = os.listdir(data_dir)\n",
    "    \n",
    "    if not filenames:\n",
    "        return 0\n",
    "    \n",
    "    return max([\n",
    "        int(filename.split('.')[0][2:]) for filename in filenames\n",
    "    ]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e23c7-8209-4400-90ba-fa0de74ec11c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f984fb826244dfac4995ca5942f4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = os.path.join(\n",
    "    PROJECT_DIR,\n",
    "    \"data/generated_transcriptions/\",\n",
    ")\n",
    "resuming_index = get_resuming_index(data_dir)\n",
    "\n",
    "for i in tqdm(range(resuming_index, len(full_dataset))):\n",
    "    full_transcription = transcripe_episode(\n",
    "        episode=full_dataset['audio'][i],\n",
    "        processor=processor,\n",
    "        model=model,\n",
    "        skip_special_tokens=True,\n",
    "        minutes=0.4, ## due to model output constraint\n",
    "        target_sampling_rate=16_000,\n",
    "    )\n",
    "\n",
    "    path = os.path.join(\n",
    "        PROJECT_DIR,\n",
    "        \"data/generated_transcriptions/\",\n",
    "        f\"ep{i}.txt\"\n",
    "    )\n",
    "\n",
    "    with open(path, 'w', encoding=\"utf-8\") as file:\n",
    "        file.write(full_transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae373b9-a780-4b52-9d88-288c3f30a29b",
   "metadata": {},
   "source": [
    "Since this is very costly, we will rely on already present transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eddc834-f5f5-496b-9bde-ec517ff1ae53",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chunking Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5598b3-8fc4-43cf-a329-7840b359b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    full_dataset = full_dataset.remove_columns(['audio'])\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b822e2-6749-463b-8fe3-344ccc4cb235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_large_text(text, max_chunk_size=1000):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        sentence = sentences[i]\n",
    "        if '?' in sentence.strip():\n",
    "            # Handle the question and its following answer\n",
    "            question_chunk = sentence\n",
    "            i += 1\n",
    "            # Include following sentences as the answer, ensuring not to exceed the max_chunk_size\n",
    "            while i < len(sentences) and len(question_chunk) + len(sentences[i]) <= max_chunk_size:\n",
    "                question_chunk += \" \" + sentences[i]\n",
    "                i += 1\n",
    "            # Add the combined question-answer chunk to the list\n",
    "            chunks.append(question_chunk.strip())\n",
    "        else:\n",
    "            # If the current chunk can accommodate the sentence\n",
    "            if len(current_chunk) + len(sentence) <= max_chunk_size:\n",
    "                current_chunk += sentence + \" \"\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "            i += 1\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def preindex_process_text(\n",
    "    episode, \n",
    "    chunking_function,\n",
    "    **chunking_function_params,\n",
    "):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    if 'audio' in episode:\n",
    "        del episode['audio']\n",
    "    if 'description' in episode:\n",
    "        del episode['description']\n",
    "    if 'segments' in episode:\n",
    "        del episode['segments']\n",
    "\n",
    "    text = episode['text']\n",
    "    del episode['text']\n",
    "\n",
    "    chunks = chunking_function(\n",
    "        text,\n",
    "        **chunking_function_params,\n",
    "    )\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        episode_doc = episode.copy()\n",
    "\n",
    "        episode_doc['text'] = chunk\n",
    "        episode_doc['chunk_id'] = i\n",
    "\n",
    "        documents.append(episode_doc)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b793bf9-64f3-44ec-80c8-255142c938a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cd4ce0456446b9877b3af05157e515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/346 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.multithread import map_progress\n",
    "\n",
    "documents = map_progress(\n",
    "    f=lambda episode:preindex_process_text(\n",
    "        episode=episode, \n",
    "        chunking_function=chunk_large_text,\n",
    "        max_chunk_size=2000,\n",
    "    ),\n",
    "    seq=full_dataset,\n",
    "    max_workers=4\n",
    ")\n",
    "\n",
    "documents = [item for sublist in documents for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a761b900-8415-4861-9dab-5764b2eb7fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /mnt/workspace/__ing/llming/DTC/audio_podcast_qa_assistant/data/generated_documents/documents.json\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(\n",
    "    PROJECT_DIR,\n",
    "    \"data/generated_documents/\",\n",
    "    \"documents.json\"\n",
    ")\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "if not os.path.exists(path):\n",
    "    with open(path, 'w') as json_file:\n",
    "        json.dump(documents, json_file, indent=4)\n",
    "\n",
    "    print(f\"Data successfully saved to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62473643-15e0-4bbe-ad05-c722eb4a179f",
   "metadata": {},
   "source": [
    "# Questions Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eced7797-d848-4fe7-91ae-74e169abb40c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    full_dataset = full_dataset.remove_columns(['audio'])\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b614649d-ba48-4d33-8193-dec719c33203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_questions(\n",
    "    episode,\n",
    "    min_words = 11,\n",
    "):\n",
    "    questions = []\n",
    "    epidose_id = episode['id']\n",
    "    \n",
    "    for segment in episode['segments']:\n",
    "        if '?' == segment['text'][-1]:\n",
    "            segment_text_list = segment['text'].split()\n",
    "            if segment_text_list[0].istitle():\n",
    "                if len(segment_text_list) >= min_words:\n",
    "                    questions.append(\n",
    "                        {\n",
    "                            \"episode_id\": epidose_id,\n",
    "                            \"question\": segment['text']\n",
    "                        }\n",
    "                    )\n",
    "                \n",
    "    return questions\n",
    "\n",
    "\n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "\n",
    "def sample_from_list(\n",
    "    orig_list,\n",
    "    sample_size=None,\n",
    "    seed=42,\n",
    "):\n",
    "    indices = np.arange(0, len(orig_list))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    return list(np.array(orig_list)[indices])[:sample_size]\n",
    "\n",
    "\n",
    "def group_questions_by_episode(questions):\n",
    "    questions_per_episode = defaultdict(list)\n",
    "    for question in questions:\n",
    "        questions_per_episode[question['episode_id']].append(question)\n",
    "\n",
    "    questions_per_episode = list(questions_per_episode.values())\n",
    "    return questions_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c3d5ca50-5c2a-4252-9619-4eb560a09ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd5216af6aa42ab96e77c35f79b380e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/346 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions = map_progress(\n",
    "    f=lambda episode:extract_questions(\n",
    "        episode=episode,\n",
    "        min_words=15,\n",
    "    ),\n",
    "    seq=full_dataset,\n",
    "    max_workers=4\n",
    ")\n",
    "\n",
    "questions = flatten_list_of_lists(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1e494c61-6069-4de6-bf5d-9a5188e6aaf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions before sampling: 11136\n",
      "Number of questions after sampling: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of questions before sampling:\", len(questions))\n",
    "questions = sample_from_list(questions, sample_size=500)\n",
    "print(\"Number of questions after sampling:\", len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8b8e7cf4-d4c1-4bdd-8c5d-710811772910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group questions by episode_id\n",
    "questions_per_episode = group_questions_by_episode(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca35b73-e95a-4707-ac82-94b1842afe72",
   "metadata": {},
   "source": [
    "# Openai rephrase questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c978e49d-d325-49fd-ad1e-d88ae1b51e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.openai import create_openai_client\n",
    "from utils.utils import parse_json_response\n",
    "\n",
    "openai_client = create_openai_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7b648318-5a11-4aa9-a09f-d677417ec9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def openai_rephrase(episode_questions, model=\"gpt-4o-mini\"):\n",
    "    prompt = \"\"\"\n",
    "I have a list of questions extracted from various episodes. \n",
    "Each question is associated with an `episode_id`. \n",
    "Your task is to check if each question is self-sufficient, meaning it has enough context to stand alone. \n",
    "If a question is self-sufficient, rephrase it to convey the same meaning clearly. \n",
    "If a question is not self-sufficient and lacks necessary context, \n",
    "do not rephrase it nor return it, just skip it, be very strict in this point, the question needs to answerable on its own.\n",
    "\n",
    "Please return the results in a JSON format, with each question represented as a dictionary containing `episode_id` and `question` keys.\n",
    "\n",
    "IMPORTANT: Only return the dictionary without the fenced code block marks. Make sure to return double quotations not single quotations.\n",
    "\n",
    "Here is the list of questions:\n",
    "    \"\"\".strip() + f\"\\n{str(episode_questions)}\"\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Extract and print the response\n",
    "    return parse_json_response(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dab8b3bb-9d88-4482-b1d2-9defa2711069",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a974de08e23241bb8175d8735d935073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rephrased_questions = map_progress(\n",
    "    f=lambda episode_questions:openai_rephrase(\n",
    "        episode_questions=episode_questions,\n",
    "        model=\"gpt-4o-mini\",\n",
    "    ),\n",
    "    seq=questions_per_episode,\n",
    "    max_workers=4,\n",
    ")\n",
    "\n",
    "rephrased_questions = flatten_list_of_lists(rephrased_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cd703c97-bdea-4fb7-87e3-cfb234428c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'episode_id': 'IT__Nrr3PNI',\n",
       "  'question': 'How do irrational numbers relate to programming?'},\n",
       " {'episode_id': 'LRYkH-fAVGE',\n",
       "  'question': 'How would you define the general problem of computer vision?'},\n",
       " {'episode_id': 'iZRbD7q1n-U',\n",
       "  'question': 'What are the current weaknesses in a specific combat sport?'},\n",
       " {'episode_id': 'Pl3x4GINtBQ',\n",
       "  'question': 'In a sense, are there multiple governments or systems of enforcement?'},\n",
       " {'episode_id': 'LRYkH-fAVGE',\n",
       "  'question': 'In the context of driving, is vision the primary challenge, or do actions and interactions with the environment also play a crucial role?'}]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_from_list(rephrased_questions, sample_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9994819a-947d-4cfb-bc0b-23d690803ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions after discarding non-self-sufficient: 389\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of questions after discarding non-self-sufficient:\", len(rephrased_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c2e69062-91cb-42db-9750-3a4abaac46e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /mnt/workspace/__ing/llming/DTC/audio_podcast_qa_assistant/data/generated_questions/questions.json\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(\n",
    "    PROJECT_DIR,\n",
    "    \"data/generated_questions/\",\n",
    "    \"questions.json\"\n",
    ")\n",
    "\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    with open(path, 'w') as json_file:\n",
    "        json.dump(rephrased_questions, json_file, indent=4)\n",
    "\n",
    "    print(f\"Data successfully saved to {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 (dtc-llm-env)",
   "language": "python",
   "name": "dtc-llm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
